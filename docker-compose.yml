# main/target DB where the data will be ingested
services:
  postgres_warehouse:
    image: postgres:13
    environment:
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
      POSTGRES_DB: user_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
# metabase db
  metabase:
    image: metabase/metabase:latest
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: metabase
      MB_DB_PORT: 5432
      MB_DB_USER: myuser
      MB_DB_PASS: mypassword
      MB_DB_HOST: postgres_warehouse
    depends_on:
      - postgres_warehouse
# airflow webserver. additional parameters are added with aim to increase import/execution time
  airflow-webserver:
    build: .
    command: webserver
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here
      - AIRFLOW_HOME=/opt/expdir/airflow
      - AIRFLOW__CORE__DAG_CONCURRENCY=1
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=60
      - AIRFLOW__CORE__DAG_FILE_PROCESSOR_TIMEOUT=300
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=600
      - AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC=60
      - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=60
      - AIRFLOW__SCHEDULER__RUN_DURATION=-1
      - AIRFLOW__SCHEDULER__NUM_RUNS=-1
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    volumes:
      - ./dags:/opt/expdir/airflow/dags
      - ./data:/opt/expdir/data
    depends_on:
      - airflow-db
      - airflow-init
# airflow scheduler
  airflow-scheduler:
    build: .
    command: scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here
      - AIRFLOW_HOME=/opt/expdir/airflow
      - AIRFLOW__CORE__DAG_CONCURRENCY=1
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=60
      - AIRFLOW__CORE__DAG_FILE_PROCESSOR_TIMEOUT=300
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=600
      - AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC=60
      - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=60
      - AIRFLOW__SCHEDULER__RUN_DURATION=-1
      - AIRFLOW__SCHEDULER__NUM_RUNS=-1
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    volumes:
      - ./dags:/opt/expdir/airflow/dags
      - ./data:/opt/expdir/data
    depends_on:
      - airflow-db
      - airflow-init
# airflow fb
  airflow-db:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
# airflow init
  airflow-init:
    build: .
    command: db init
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here
      - AIRFLOW_HOME=/opt/expdir/airflow
    depends_on:
      - airflow-db
# volumes setup
volumes:
  postgres_data:
  airflow_db_data: